{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "For me to better understand the Transformer architecture and how to train them, I designed this simple sequence2sequence example which takes in a string and sorts in alphabetical order. \n",
        "\n",
        "It learns to do the job well within 4 epochs. Here is the output on unseen data after 4 epochs:\n",
        "\n",
        "Epoch: 4, Train loss: 0.10, Validation loss: 0.05\n",
        "\n",
        "Input string: sortingtest - Sorted string: eginorssttt\n",
        "\n",
        "Input string: abcdef - Sorted string: abcdef\n",
        "\n",
        "Input string: fedcba - Sorted string: abcdef\n",
        "\n",
        "Input string: prithvi - Sorted string: hiiprtv"
      ],
      "metadata": {
        "id": "73etTA5A1Rnz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "kqJ8c6GVm07s"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import string\n",
        "import itertools\n",
        "from torch.utils.data import DataLoader\n",
        "import math\n",
        "\n",
        "# Constants\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "MAX_LENGTH = 20\n",
        "BATCH_SIZE = 64\n",
        "TRAIN_SAMPLES = 50000\n",
        "VALIDATION_SAMPLES = 500\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3\n",
        "HIDDEN_SIZE = 256\n",
        "NHEAD = 4\n",
        "DROPOUT = 0.1\n",
        "\n",
        "# Data Preparation\n",
        "def generate_sample():\n",
        "    sequence_length = random.randint(1, MAX_LENGTH)\n",
        "    sequence = \"\".join(random.choices(string.ascii_lowercase, k=sequence_length))\n",
        "    sorted_sequence = \"\".join(sorted(sequence))\n",
        "    return sequence, sorted_sequence\n",
        "\n",
        "def pad_sequence(seq, max_length):\n",
        "    return seq + [0] * (max_length - len(seq))\n",
        "\n",
        "def tensor_from_string_padded(s, char_to_idx, max_length):\n",
        "    idxs = [char_to_idx[c] for c in s]\n",
        "    padded_idxs = pad_sequence(idxs, max_length)\n",
        "    return torch.tensor(padded_idxs, dtype=torch.long, device=DEVICE)\n",
        "\n",
        "def tensors_from_pair_padded(input_str, output_str, char_to_idx, max_length):\n",
        "    input_tensor = tensor_from_string_padded(input_str, char_to_idx, max_length)\n",
        "    output_tensor = tensor_from_string_padded(output_str, char_to_idx, max_length)\n",
        "    return input_tensor, output_tensor\n",
        "\n",
        "def generate_data_padded(num_samples, char_to_idx, max_length):\n",
        "    data = [generate_sample() for _ in range(num_samples)]\n",
        "    return [tensors_from_pair_padded(input_str, output_str, char_to_idx, max_length) for input_str, output_str in data]\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    inputs, targets = zip(*batch)\n",
        "    inputs = torch.stack(inputs, dim=1)\n",
        "    targets = torch.stack(targets, dim=1)\n",
        "    return inputs, targets\n",
        "\n",
        "chars = string.ascii_lowercase\n",
        "char_to_idx = {c: i + 1 for i, c in enumerate(chars)}\n",
        "idx_to_char = {i + 1: c for i, c in enumerate(chars)}\n",
        "\n",
        "train_data = generate_data_padded(TRAIN_SAMPLES, char_to_idx, MAX_LENGTH)\n",
        "validation_data = generate_data_padded(VALIDATION_SAMPLES, char_to_idx, MAX_LENGTH)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn)\n",
        "validation_loader = DataLoader(validation_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.src_mask = None\n",
        "        self.ninp = ninp\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        self.pos_encoder = PositionalEncoding(ninp)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.decoder = nn.Linear(ninp, ntoken)\n",
        "        self.init_weights()\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src, apply_softmax=False):\n",
        "        src_mask = (src != 0).permute(1,0)\n",
        "        src_mask = src_mask.to(torch.float32)\n",
        "        src_mask = src_mask.masked_fill(src_mask == 0, float('-inf'))\n",
        "        src_mask = src_mask.masked_fill(src_mask == 1, 0)\n",
        "\n",
        "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, src_key_padding_mask = src_mask)\n",
        "        output = self.decoder(output)\n",
        "\n",
        "        if apply_softmax:\n",
        "          output = torch.softmax(output, dim=-1)\n",
        "        return output\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=MAX_LENGTH):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(0), :]\n",
        "\n",
        "\n",
        "# Training\n",
        "def train_step(model, train_loader, optimizer, criterion, epoch):\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    for i, (input, target) in enumerate(train_loader):\n",
        "        input, target = input.to(DEVICE), target.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(input)\n",
        "        loss = criterion(output.view(-1, len(chars) + 1), target.reshape(-1))\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def evaluate(model, data_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    with torch.no_grad():\n",
        "        for input, target in data_loader:\n",
        "            input, target = input.to(DEVICE), target.to(DEVICE)\n",
        "            output = model(input)\n",
        "            loss = criterion(output.view(-1, len(chars) + 1), target.reshape(-1))\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "def sort_string(input_str, model, char_to_idx, idx_to_char, max_length):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensor_from_string_padded(input_str, char_to_idx, max_length).unsqueeze(1)\n",
        "        output_tensor = model(input_tensor, apply_softmax=True).squeeze(1)  \n",
        "\n",
        "        sorted_indices = torch.argmax(output_tensor, dim=-1).squeeze()\n",
        "\n",
        "        sorted_str = \"\".join([idx_to_char[idx.item()] for idx in sorted_indices if idx.item() != 0]) \n",
        "\n",
        "    return sorted_str\n",
        "\n",
        "model = TransformerModel(len(chars) + 1, HIDDEN_SIZE, NHEAD, HIDDEN_SIZE, NUM_ENCODER_LAYERS, DROPOUT).to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train_loss = train_step(model, train_loader, optimizer, criterion, epoch)\n",
        "    validation_loss = evaluate(model, validation_loader, criterion)\n",
        "    print('******************')\n",
        "    print(f'Epoch: {epoch}, Train loss: {train_loss:.2f}, Validation loss: {validation_loss:.2f}')\n",
        "    for input_str in ['sortingtest', 'abcdef', 'fedcba', 'prithvi']:\n",
        "      sorted_str = sort_string(input_str, model, char_to_idx, idx_to_char, MAX_LENGTH)\n",
        "      print(f\"Input string: {input_str} - Sorted string: {sorted_str}\")\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), 'sequence_transformer.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laQzk0Zq21WU",
        "outputId": "7aec0cfe-134f-4625-8430-ac1b55856cab"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "******************\n",
            "Epoch: 1, Train loss: 0.36, Validation loss: 0.10\n",
            "Input string: sortingtest - Sorted string: eginorssttt\n",
            "Input string: abcdef - Sorted string: abcdef\n",
            "Input string: fedcba - Sorted string: abcdef\n",
            "Input string: prithvi - Sorted string: hiiprtv\n",
            "******************\n",
            "Epoch: 2, Train loss: 0.16, Validation loss: 0.11\n",
            "Input string: sortingtest - Sorted string: eginrrssttt\n",
            "Input string: abcdef - Sorted string: abcdef\n",
            "Input string: fedcba - Sorted string: abceef\n",
            "Input string: prithvi - Sorted string: hiiprtv\n",
            "******************\n",
            "Epoch: 3, Train loss: 0.12, Validation loss: 0.07\n",
            "Input string: sortingtest - Sorted string: eginorssttt\n",
            "Input string: abcdef - Sorted string: abcdef\n",
            "Input string: fedcba - Sorted string: abcdef\n",
            "Input string: prithvi - Sorted string: hiiprtv\n",
            "******************\n",
            "Epoch: 4, Train loss: 0.10, Validation loss: 0.05\n",
            "Input string: sortingtest - Sorted string: eginorssttt\n",
            "Input string: abcdef - Sorted string: abcdef\n",
            "Input string: fedcba - Sorted string: abcdef\n",
            "Input string: prithvi - Sorted string: hiiprtv\n",
            "******************\n",
            "Epoch: 5, Train loss: 0.08, Validation loss: 0.06\n",
            "Input string: sortingtest - Sorted string: eginorssttt\n",
            "Input string: abcdef - Sorted string: abceff\n",
            "Input string: fedcba - Sorted string: abceef\n",
            "Input string: prithvi - Sorted string: hiiprtv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bbf63Unohctm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}